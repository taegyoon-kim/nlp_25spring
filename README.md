# HSS 510 / DS 518: NLP for Humanities and Social Sciences

**Spring 2025**  
**Tue 10:00am‚Äì1:00pm**  
**N4 1209-1, School of Digital Humanities and Computational Social Sciences**

## Instructor
- **Name:** Taegyoon Kim, Ph.D. in Political Science and Social Data Analytics
- **Email:** [taegyoon@kaist.ac.kr](mailto:taegyoon@kaist.ac.kr)
- **Office hours:** Wed 10:00am‚Äì12:00pm & By appointment, N4 1308
- **Webpage:** [https://taegyoon-kim.github.io](https://taegyoon-kim.github.io)

## Course Overview

This course introduces students to the fundamental concepts and techniques of NLP/text-as-data, emphasizing the development of critical insights for its application in humanities and social sciences research. The course will blend theoretical understanding with practical, hands-on experience. Students will develop not only a mathematical/statistical intuition for key NLP/text-as-data approaches but also learn how to effectively implement these approaches in their own research. 

Each class will start with a lecture by the instructor, complemented by guided coding. The latter portion of the class will feature two activities. Students will first engage in a review of applied research. Then, they will lead hands-on methods tutorials. 

While prior experience in NLP/text-as-data is not required, students should possess basic programming skills in Python, along with some familiarity with quantitative analysis. By the end of this course, students will gain a comprehensive understanding of the potential of NLP/text-as-data in humanities and social sciences, mastering techniques to apply and refine NLP/text-as-data for their research.

## Prerequisites
Basic programming skills in Python and familiarity with quantitative analysis are required

## Readings
Students will engage with articles, supplemented by the following textbooks (sections provided weekly):

- **[GRS]** Grimmer, J., Roberts, M.E., & Stewart, B.M. (2022). *Text as Data: A New Framework for Machine Learning and the Social Sciences*. [Link](https://product.kyobobook.co.kr/detail/S000002751294)

- **[JM]** Jurafsky, D. & Martin, J.H. *Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition*. [Link](https://web.stanford.edu/~jurafsky/slp3/)

- **[AG]** Alammar, J. & Grootendorst, M. (2024). *Hands-On Large Language Models: Language Understanding and Generation*. [Link](https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961)

## Summary of Major Tasks

Students are expected to complete the following tasks (the numbers in parentheses are grade values).

- **Attendance (10%)**: Attend all lectures unless excused by the instructor. Two points are deducted per absence. Late arrivals beyond 20 minutes count as absences. Students are assumed to have completed assigned readings and expected to actively participate in class.

- **Application Review Discussion (20%)**: Students (in groups) will present and lead a discussion on an applied NLP/text-as-data article.

- **Methods Tutorial (15%)**: Students (in groups) will present a hands-on methods tutorial using their own (or publicly available) data.
    
- **Research Paper (40%)**: Students will write a research paper (2,000‚Äì4,000 words).

- **Exercises (15%)**: Take-home exercises will be assigned to strengthen understanding of concepts and enhance programming skills.
  
## Application Review Discussion Details

Students (in groups) will present a review of an applied article that employs NLP/text-as-data and lead a discussion on it. This exercise is designed to enhance their understanding not only of the techniques themselves but also of their principled applications in actual research.

- Present a review of an applied NLP/text-as-data article, including:
    - Research objectives/questions.
    - Text data used and collection methods.
    - NLP/text-as-data methods applied.
    - Pros/cons of the use of the methods.
    - Suggestions for improvement.
- The presentation should last up to 15 minutes and will be followed by a 15-minute open discussion led by the presenters.
- Non-presentings students are required to acively engage in the open disucssion. <br>
- üóìÔ∏è Sign up for a slot [here](https://docs.google.com/spreadsheets/d/1QpepppII8QjuzLN3CfQ6R7xqnmPmb6KGyEXpMhVz480/edit?usp=sharing) (the **applied_review_discussion** tab). 
- üì§ Prior to the presentation, upload your presentation slides to the **applied_review_discussion_slides** folder [here](https://drive.google.com/drive/folders/1w3xBhbPXvhQsElDQISanHKIz9eRAJSiP?usp=share_link).

## Methods Tutorial Details

Students (in groups) will be responsible for presenting a methods tutorial. The presentations serve a dual purpose: first, to provide non-presenting students with a hands-on implementation of the techniques covered in the previous week; and second, to offer presenting students an opportunity to apply and adapt those techniques in their own research.

- Present and demonstrate the implementation of NLP/text-as-data techniques using real data (preferably your own data).
- Walk the class through the script, explaining each step and ensuring everyone can follow.
- The presentation should last up to 15 minutes.
- üóìÔ∏è Sign up for a slot [here](https://docs.google.com/spreadsheets/d/1QpepppII8QjuzLN3CfQ6R7xqnmPmb6KGyEXpMhVz480/edit?usp=sharing) (the **methods_tutorial** tab).
- üì§ Prior to the presentation, upload yuor tutorial materials (scripts, datasets) to the **methods_tutorial_materials** folder [here](https://drive.google.com/drive/folders/1w3xBhbPXvhQsElDQISanHKIz9eRAJSiP?usp=share_link).

## Research Paper Details

The research paper may either apply NLP/text-as-data to answer a substantive or theoretical question in your field of research or contribute methodologically to the literature on NLP/text-as-data. The objective is to use this as an opportunity to write (and eventually publish) a substantive research paper ‚Äî not just a course paper for the sake of fulfilling a requirement!

- The paper should follow a format that includes a title, abstract, main text, references, and appendices or supplementary materials.
- The expected length is 2,000‚Äì-4,000 words, although longer papers are also accepted.
- Papers must be suitable for submission to a peer-reviewed journal.
- As an intermediate step, submit a one-page proposal (yes, *one* page) by April 18.
- üóìÔ∏è Sign up for a one-on-one meeting to discuss your proposal [here](https://docs.google.com/spreadsheets/d/1QpepppII8QjuzLN3CfQ6R7xqnmPmb6KGyEXpMhVz480/edit?usp=sharing) (the **one_on_one_meeting** tab).
- üì§ Upload your one-page proosal to the **one_page_proposals** folder [here](https://drive.google.com/drive/folders/1w3xBhbPXvhQsElDQISanHKIz9eRAJSiP?usp=share_link) by April 18. 
- üì§ Upload your final papers to the **final_papers** folder [here](https://drive.google.com/drive/folders/1w3xBhbPXvhQsElDQISanHKIz9eRAJSiP?usp=share_link) by June 20.
  
## Weekly Schedule

The weekly schedule may be modified as needed to align with the course's overall progress and students' varying levels of comprehension. 1) The article marked with a cross (‚úù) is designated for application review. 2) Some courses will be held via Zoom, with students participating individually (not together in the classroom). 3) Be sure to bring your laptop for every class.

### Week 1 (Feb 25): Course Overview
  - Objectives, class components, weekly themes  
  - Install Python before the next class  

### Week 2 (Mar 4): Selecting and Cleaning Texts
- Required reading
  - [GRS] Sections 3.1 and 3.2 in Chp. 3 "Principles of Selection and Representation." 
  - [GRS] Chp. 4 "Selecting Documents."
  - [JM] Sections 2.1 and 2.2 in Chp. 2 "Regular Expressions, Text Normalization, Edit Distance"
  - [AG] pp. 37-56 in Chp. 2 "Tokens and Embeddings"

### Week 3 (Mar 11): Representing and Comparing Texts  
- Required reading
  - [GRS] Chp. 5 "Bag of Words."
  - [GRS] Chp. 7 "The Vector Space Model and Similarity Metrics."
  - ‚úù Denny, M.J. and Spirling, A., 2018. Text preprocessing for unsupervised learning: Why it matters, when it misleads, and what to do about it. Political Analysis, 26(2), pp.168-189.
- Optional reading
  - [GRS] Sections 3.3 and 3.4 in Chp. 3 "Principles of Selection and Representation." 
  - [JM] Section 2.4 in Chp. 2 "Regular Expressions, Text Normalization, Edit Distance"
  - Christopher, D., Raghavan, P. and Sch√ºtze, H., 2008. Scoring term weighting and the vector space model. Introduction to information retrieval, 100, pp.2-4.

### Week 4 (Mar 18): Supervised Learning Methods I  
- Required reading
  - [GRS] Chp. 17 "An Overview of Supervised Classification"
  - [GRS] Chp. 18 "Coding a Training Set"
  - [GRS] Chp. 19 "Classifying Documents with Supervised Learning"
  - [GRS] Chp. 20 "Checking Performance"
  - ‚úù Siegel, A.A., Nikitin, E., Barber√°, P., Sterling, J., Pullen, B., Bonneau, R., Nagler, J. and Tucker, J.A., 2021. Trumping hate on Twitter? Online hate speech in the 2016 US election campaign and its aftermath. Quarterly Journal of Political Science, 16(1), pp.71-104.
- Optional reading
  - Piper, A., 2022. Biodiversity is not declining in fiction. Journal of Cultural Analytics, 7(3).

### Week 5 (Mar 25): Supervised Learning Methods II  
- Required reading
  - [JM] Chp. 5 "Logistic Regression"
  - [JM] Sections 7.1-7.4 and 7.6 in Chp. 7 "Neural Networks and Neural Language Models"
  - Barber√°, P., Boydstun, A.E., Linn, S., McMahon, R. and Nagler, J., 2021. Automated text classification of news articles: A practical guide. Political Analysis, 29(1), pp.19-42.
  - ‚úù Bestvater, S.E. and Monroe, B.L., 2023. Sentiment is not stance: Target-aware opinion classification for political text analysis. Political Analysis, 31(2), pp.235-256.
- Optional reading
  - [JM] Chp. 4 "Naive Bayes and Sentiment Classification"
  - Arnold, C., Biedebach, L., K√ºpfer, A. and Neunhoeffer, M., 2023. The role of hyperparameters in machine learning models and how to tune them. Political Science Research and Methods, pp.1-8.

### Week 6 (Apr 1): Embeddings  
- Required reading
  - [GRS] Chp. 8 "Distributed Representations of Words"
  - [JM] Chp. 6 ``Vector Semantics and Embeddings"
  - [AG] pp. 57-71 in Chp. 2 "Tokens and Embeddings"
  - Rodriguez, P.L. and Spirling, A., 2022. Word embeddings: What works, what doesn‚Äôt, and how to tell the difference for applied research. The Journal of Politics, 84(1), pp.101-115.
  - ‚úù Kozlowski, A.C., Taddy, M. and Evans, J.A., 2019. The geometry of culture: Analyzing the meanings of class through word embeddings. American Sociological Review, 84(5), pp.905-949.
- Optional reading
  - Soni, S., Klein, L.F. and Eisenstein, J., 2021. Abolitionist networks: Modeling language change in nineteenth-century activist newspapers. Journal of Cultural Analytics, 6(1).
  - Caliskan, A., Bryson, J.J. and Narayanan, A., 2017. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334), pp.183-186.
  - Osnabr√ºgge, M., Hobolt, S.B. and Rodon, T., 2021. Playing to the gallery: Emotive rhetoric in parliaments. American Political Science Review, 115(3), pp.885-899.
  - Garten, J., Hoover, J., Johnson, K.M., Boghrati, R., Iskiwitch, C. and Dehghani, M., 2018. Dictionaries and distributions: Combining expert knowledge and large scale textual data content analysis: Distributed dictionary representation. Behavior research methods, 50, pp.344-361.
  - Pennington, J., Socher, R. and Manning, C.D., 2014, October. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543).
    
### Week 7 (Apr 8): Topic Models
- Required reading
  - [GRS] Chp. 13 ``Topic Models."
  - Blei, D.M., 2012. Probabilistic topic models. Communications of the ACM, 55(4), pp.77-84.
  - ‚úù Barber√°, P., Casas, A., Nagler, J., Egan, P.J., Bonneau, R., Jost, J.T. and Tucker, J.A., 2019. Who leads? Who follows? Measuring issue attention and agenda setting by legislators and the mass public using social media data. American Political Science Review, 113(4), pp.883-901.
- Optional reading
  - Maier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., H√§ussler, T. and Schmid-Petri, H., 2021. Applying LDA topic modeling in communication research: Toward a valid and reliable methodology. In Computational methods for communication science (pp. 13-38). Routledge.
  - Ying, L., Montgomery, J.M. and Stewart, B.M., 2022. Topics, concepts, and measurement: A crowdsourced procedure for validating topics as measures. Political Analysis, 30(4), pp.570-589.
  - Roberts, M.E., Stewart, B.M., Tingley, D. and Airoldi, E.M., 2013, December. The structural topic model and applied social science. In Advances in neural information processing systems workshop on topic models: computation, application, and evaluation (Vol. 4, No. 1, pp. 1-20).

### Week 8 (Apr 15): Mid-term Break
- No class

### Week 9 (Apr 22): NLP with Korean [[Zoom link]](https://kaist.zoom.us/my/taegyoon)
- (10:00-11:15am) Guest lecture by [Byungjoon Kim](https://byungjunkim.com) on NLP with Korean  

### Week 10 (Apr 29): One-on-One Meetings 
- One-on-one meetings will be based on one-page proposals and will occur on Apr 22, 23, and 30 as well as Apr 29.

### Week 11 (May 6): Children's Day
- No class

### Week 12 (May 13): Representation Models I
- Required reading
  - Smith, N.A., 2019. Contextual word representations: A contextual introduction. arXiv preprint arXiv:1902.06006.
  - [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
  - [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
  - [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning](https://jalammar.github.io/illustrated-bert/)
  - Grootendorst, M., 2022. BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv preprint arXiv:2203.05794.
  - ‚úù Falkenberg, M., Galeazzi, A., Torricelli, M., Di Marco, N., Larosa, F., Sas, M., Mekacher, A., Pearce, W., Zollo, F., Quattrociocchi, W. and Baronchelli, A., 2022. Growing polarization around climate change on social media. Nature Climate Change, 12(12), pp.1114-1121.
- Optional reading
  - [JM] Sections 7.1-7.4 and 7.6 in Chp. 7 "Neural Networks and Neural Language Models"
  - [JM] Chp. 9 "RNNs and LSTMs"
  - [JM] Chp. 10 ``Transformers and Pretrained Language Models"
  - Rogers, A., Kovaleva, O. and Rumshisky, A., 2021. A primer in BERTology: What we know about how BERT works. Transactions of the Association for Computational Linguistics, 8, pp.842-866.

### Week 13 (May 20): Representation Models II 
- Required reading
  - [AG] Chp. 11 "Fine-tuning Representation Models for Classification"
  - [JM] Chp. 11 "Fine-tuning and Masked Language Models"
  - ‚úù Card, D., Chang, S., Becker, C., Mendelsohn, J., Voigt, R., Boustan, L., Abramitzky, R. and Jurafsky, D., 2022. Computational analysis of 140 years of US political speeches reveals more positive but increasingly polarized framing of immigration. Proceedings of the National Academy of Sciences, 119(31), p.e2120510119.

- Optional reading
  - Wang, Y. (2024). On finetuning large language models. Political Analysis, 32(3), 379-383.
  - Laurer, M., Van Atteveldt, W., Casas, A. and Welbers, K., 2024. Widmann, T. and Wich, M., 2023. Creating and comparing dictionary, word embedding, and transformer-based models to measure discrete emotions in German political text. Political Analysis, 31(4), pp.626-641.
  - Nguyen, T. D., Chen, Z., Carroll, N. G., Tran, A., Klein, C., & Xie, L. (2024, May). Measuring moral dimensions in social media with mformer. In Proceedings of the International AAAI Conference on Web and Social Media (Vol. 18, pp. 1134-1147).
    
### Week 14 (May 27): Large Language Models I  
- Required reading
  - [AG] Chp. 1 "An Introduction to Large Language Models"
  - [AG] Chp. 3 "Looking Inside Large Language Models"
  - T√∂rnberg, P. (2023). How to use Large Language Models for Text Analysis. arXiv preprint arXiv:2307.13106.
  - T√∂rnberg, P. (2024). Best practices for text annotation with large language models. arXiv preprint arXiv:2402.05129.
  - ‚úù Barrie, C., Palaiologou, E., & T√∂rnberg, P. (2024). Prompt stability scoring for text annotation with large language models. arXiv preprint arXiv:2407.02039.
- Optional reading
  - Hussain, Z., Binz, M., Mata, R., & Wulff, D. U. (2024). A tutorial on open-source large language models for behavioral science. Behavior Research Methods, 56(8), 8214-8237.

### Week 15 (Jun 3): Large Language Models II  
- Required reading
  - [AG] Chp. 6 "Prompt Engineering"
  - [AG] Chp. 12 "Fine-tuning Generation Models"
  - Ornstein, J. T., Blasingame, E. N., & Truscott, J. S. (2023). How to train your stochastic parrot: Large language models for political texts. Political Science Research and Methods, 1-18.
  - Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., ... & Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35, 24824-24837.
  - Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., ... & Zhou, D. (2022). Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.

### Week 16 (Jun 10): Writing Research Paper
- No class


## Policies
- **Instruction Mode:** In-person, with potential changes announced in advance.
- **Email Policy:** Emails are typically answered within two business days.
- **Late Submissions:** 10% penalty per day late.
- **Syllabus Changes:** The syllabus may be adjusted to accommodate students' progress and needs.

## Academic Integrity
As students at KAIST, you are entrusted with upholding the utmost standards of academic integrity. Academic honesty is paramount, and any form of misconduct is strictly prohibited. In the event of suspected misconduct, our course adheres to the established policy of KAIST. 

## Grading Scale
| Grade | Lower | Upper |
|-------|-------|-------|
| A+    | 90    | 101   |
| A‚ÇÄ    | 87    | 90    |
| A-    | 84    | 87    |
| B+    | 81    | 84    |
| B‚ÇÄ    | 78    | 81    |
| B-    | 75    | 78    |
| C+    | 72    | 75    |
| C‚ÇÄ    | 69    | 72    |
| C-    | 66    | 69    |
| D+    | 63    | 66    |
| D‚ÇÄ    | 60    | 63    |
| F     | 0     | 60    |
